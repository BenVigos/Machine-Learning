{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Support Vector Machines to predict stable crystal compositions\n",
    "\n",
    "In this notebook you will work with three datafiles: ex6data1.dat, ex6data2.dat and ex6data3.dat. They contain information about experiments of a three hypothetical materials: Simpilian, Complexian and Absurdian as function of their composition presented by coordinates (X1,X2). The material can either form a form a 'good' single homogeneous crystal (1) or have decomposed in a 'bad' poly-crystalline sample (-1). It is your task to find the boundary between these two phases based on a finite experimental dataset. The complexity of the decision boundary grows from example 1 to 3. The data is linearly separable in ex.1 and 2 by either a straight line or a curvy line, respectively. In example 3 you will encounter that the experimental data close to the decision boundary is diffuse or contains noise/errors.\n",
    "\n",
    "## Part 1. The Genetic Algorithm\n",
    "\n",
    "We are going to start by coding a general genetic algorithm that can be used to maximize an arbitrary fitness function. You will first test your code by maximizing \n",
    "$$f(\\boldsymbol{\\theta})=-(\\theta_0-10)^2-(\\theta_0-\\theta_1)^2-(\\theta_0-\\theta_1-\\theta_2-3)^2$$\n",
    "This is maximisation is equivalent to the **minimisation** of the squared loss function:\n",
    "$$loss(\\boldsymbol{\\theta})=(\\theta_0-10)^2-(\\theta_0-\\theta_1)^2-(\\theta_0-\\theta_1-\\theta_2-3)^2$$\n",
    "__The jargon of 'maximisation of the fitness' is coming from the analogy to genetic eveloution, in which the fittest individuals have the largest chance to procreate. However, a formulation in terms of a minimisation are usualy perferable for ML. Note that here loss=-fitness, simply. For the sake of the analogy we will use the fitness here.__\n",
    "\n",
    "Derive the solution analytically yourself. <em>Answer:</em> $\\boldsymbol{\\theta} =\\{10,10,-3\\}$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing the required modules\n",
    "%pip install numpy matplotlib seaborn pandas scikit-learn #installs them if they are not yet there\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "#import sklearn\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "#print('The scikit-learn version is {}.'.format(sklearn.__version__))\n",
    "\n",
    "# to get matplot figures render correctly in the notebook use:\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f(theta,C=1):\n",
    "    \"\"\"\n",
    "    computes the loss (-1* fitness of individual with genome theta) according to the equation\n",
    "    loss = (theta[0]-10)^2 + (theta[0]-theta[1])^2 + (theta[0]-theta[1]-theta[2]-3)^2 e\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : (3,) ndarray of float (the genome of the individual)\n",
    "    C: float: Optional number  scale the penalty in the fitness function [Ignore in this function!]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "        The loss of theta\n",
    "\n",
    "    \"\"\"\n",
    "    '''YOUR CODE GOES HERE '''\n",
    "\n",
    "    return fitnnes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "We define $K$ vectors $\\boldsymbol{\\theta}_k$, $k=0, \\ldots, K-1$ that serve all as approximations of the real maximum. Every vector in this collection has a certain fitness $f_k = f(\\boldsymbol{\\theta}_k)$. We will apply a genetical algortithm in this notebook, in which each $\\boldsymbol{\\theta}_k$ is an **indiviual**.\n",
    "\n",
    "For the time being assume these vectors are sorted according decreasing fitness. So $\\boldsymbol{\\theta}_0$ has the largest fitness $f_0$ and $\\boldsymbol{\\theta}_{K-1}$ the lowest fitness $f_{K-1}$. Genetically the vector $\\boldsymbol{\\theta}_0$ is stronger $\\boldsymbol{\\theta}_1$, etc.\n",
    "\n",
    "After initializing a population with random **genomes** and sorting them according to fitness, we are going to generate a new population, again of size $K$. (You will see that there are quite some choices to be made in the sequal, but, strangely enough, it does not matter so much how you choose them: this genetic algorithm works 'all the time'.) \n",
    "We need two functions  `cross(theta1, theta2)` and   `mutate(theta)` in order to generate a new population.\n",
    "\n",
    "- The function `cross(theta1, theta2)` combines individuals $theta1, theta2$, to a new genome/individual  \n",
    "- The function `mutate(theta)` mutates individual $theta$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "\n",
    "def cross(theta1, theta2):\n",
    "    \"\"\"\n",
    "    generates a new individual by crossing genes from individuals theta1 and theta2 \n",
    "    by taking half the genes (randomly chosen) from theta1 and the others from theta2\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta1, theta2 : (N,) ndarray of float (N is the number of genes of the genomes)\n",
    "        The two indivuals/genomes that will be crossed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    crossed_theta : (N,) ndarray of float (N is the number of genes of the genomes)\n",
    "        The new individual/genome\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    '''YOUR CODE GOES HERE '''\n",
    "\n",
    "    return crossed_theta\n",
    "\n",
    "\n",
    "def mutate(theta):\n",
    "    \"\"\"\n",
    "    mutates an individual theta by adding a randomly chosen values to randomly chosen genes \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : (N,) ndarray of float (N is the number of genes of the individual)\n",
    "        The individual/genome that gets mutated\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mutated_theta : (N,) ndarray of float (N is the number of genes of the individual)\n",
    "        The new individual/genome\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    '''YOUR CODE GOES HERE '''\n",
    "\n",
    "    return mutated_theta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Test your mutate function on theta=np.ones(3)\n",
    "mutate(np.ones(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "These functions allow us to combine genomes of individuals. We of course now need a strategy **which individuals** are going to take part in it. As a first attempt, lets test the following genetic algorithm:\n",
    "\n",
    "As the individuals with a low index are strong, we let the strongest $25\\%$ be part of the new population as well.\n",
    "\n",
    "For the other $75\\%$ we are going to combine the genomes of two parents, $A, B$, giving an individual (offspring) $C$ with  \n",
    "\n",
    "| range | A: parent 1 | B: parent 2 | C: offspring |\n",
    "|---|:---:|:---:|:-------------------------------------------------------------------------------:|\n",
    "|$$\\text{first }25\\%$$ | none |  none | $$\\boldsymbol{\\theta}_k, \\text{with }k=0..K/4-1$$ | \n",
    "|$$\\text{second }25\\%$$ | $$\\boldsymbol{\\theta}_k$$ | $$\\boldsymbol{\\theta}_{k+1}$$ | $$\\text{cross(A,B) + mutate, } k=0..K/4-1$$ |\n",
    "|$$\\text{third } 25\\%$$ | $$\\boldsymbol{\\theta}_k$$ | $$\\boldsymbol{\\theta}_r$$ | $$\\text{cross(A,B) + mutate, } k=0..K/4-1, r \\text{ is random}$$|  \n",
    "|$$\\text{fourth } 25\\%$$ | $$\\boldsymbol{\\theta}_r$$ | $$\\boldsymbol{\\theta}_s$$ | $$\\text{cross(A,B) + mutate, } r \\text{ and } s \\text{ taken random}$$ | \n",
    "\n",
    "For convenience we take $K$ to be a multiple of $4$, so it will be straightforward to split the population in the 4 groups. And again: change the rules and see if you can do better than my first attempt!\n",
    "\n",
    "The resulting algorithm may seem to be slow. However, if the number of parameters becomes (much) larger the algorithm still works more or less at the same speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cross_and_mutate(population):\n",
    "    \"\"\"\n",
    "    generates a new population of individuals by crossing and mutating \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    population : (K, N+1) ndarray of float (K is the number of individuals, N+1 is the number of genes of the individual)\n",
    "        The population used to generate the new population. The population is assumed to be in order of fitness\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    offspring : (K, N+1) ndarray of float (see above)\n",
    "        The new population\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    '''YOUR CODE GOES HERE '''\n",
    "    ## pop describes the complete population: \n",
    "    K, N = population.shape\n",
    "    offspring = np.zeros_like(population)\n",
    "    \n",
    "    for k in range(K):\n",
    "        # copy best 25% from population\n",
    "        if k < ....:\n",
    "            offspring[k,:] = population[k,:]\n",
    "        \n",
    "        # second 25%\n",
    "        \n",
    "        # third 25%\n",
    "        \n",
    "        # final 25%\n",
    "        \n",
    "        ## next mutate\n",
    "\n",
    "        \n",
    "    return offspring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run(f, K, N, num_generations = 1000, C=1000):\n",
    "    \"\"\"\n",
    "    executes the genetic algorithm to maximize a fitness function \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f: function:  returns a fitness value and has as argument an individual/genome (ndarray of shape (N,))\n",
    "    K: integer : size of the population (multiple of 4)\n",
    "    N: integer : number of genes in an individual\n",
    "    num_generations: integer : number of generation that are executed\n",
    "    C: float: Real optional number used in to scale the penalty in the fitness function\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (theta, fitness_history): tuple\n",
    "        theta: ndarray(N,) of float representing the individual.genome that yielded the best fitness\n",
    "        fitness: ndarray(num_generations, ) of float the best fitness after each generation\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    '''YOUR CODE GOES HERE '''\n",
    "\n",
    "    return(theta, fitness_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# execute the genetic algorithm\n",
    "\n",
    "K = ...  # population size (~10-100)\n",
    "N = 3  # number of genes in the genome\n",
    "num_generations = ...\n",
    "\n",
    "theta, fitness = run(f, K, N, num_generations)\n",
    "\n",
    "print(theta)\n",
    "plt.plot(fitness)  # plot the evolution of the fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "This should converge to fitness = 0 best genome  $\\boldsymbol{\\theta} = [10, 10, -3]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Part 2. The Support Vector Machine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "**We have now setup the algorithm and apply it to data sets:**\n",
    "\n",
    "- The first we do by using a package **sklearn**. \n",
    "- Second, we will solve it by using the genetic algorithm. \n",
    "- Later on you will use the package **sklearn** on more difficult data sets (to write a code that does what **sklearn** can do will take many ECs....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "The data sets below will be data in 2D with two types: $y=1$ and $y=-1$ . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Example Dataset 1: Material Simpilian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FILENAME = 'SVMdata1.txt'  # points to the datafolder; either a relative or absolute path\n",
    "DELIMITER = ','  # we use a lot of csv or txt files where the data is separated by a comma\n",
    "\n",
    "data = np.loadtxt(FILENAME, delimiter=DELIMITER)\n",
    "X1 = data[:,:2]\n",
    "y1 = data[:,2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# two plot functions are defined.\n",
    "# one for plotting the data and the decision boundary and margins of the SVM solution\n",
    "# one for plotting the data and the results from the SVM from sklearn\n",
    "\n",
    "def plotdata(X, y, theta=None):\n",
    "    \"\"\"\n",
    "    plots the data and optionally the decision boundary and margins \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: ndarray of size (M, 2) : data, with M the number of samples and 2 features\n",
    "    y: ndarray of size (M,) :  labels of the datapoints (either 1 or other)\n",
    "    theta: ndarray(3,) : solution of the SVM decision boundary theta[0] = bias, theta[1] and theta[2] are the weights\n",
    "                        if None (default) no baoundary is plotted\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    mask = y == 1\n",
    "    plt.scatter(*X[mask,:].T, s=50, c='k', marker='+', label='1')\n",
    "    plt.scatter(*X[~mask,:].T, s=50, c='y', marker='o', label='-1')\n",
    "    \n",
    "    # optionally plot decision boundary \n",
    "    if theta is not None:\n",
    "        x = np.linspace(min(X[:,0]), max(X[:,0]), 10)\n",
    "        b, w1, w2 = theta\n",
    "        y_db = -w1 / w2 * x - b / w2  # see eq 4.5 reader\n",
    "        # margins\n",
    "        y_margin1 = y_db + 1 / w2  # see eq. 4.9 reader with gamma=1\n",
    "        y_margin2 = y_db - 1 / w2\n",
    "        plt.plot(x, y_db, label='decision boundary')\n",
    "        plt.plot(x, y_margin1, ':r', label='margins')\n",
    "        plt.plot(x, y_margin2, ':r')\n",
    "        \n",
    "    plt.legend()\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "\n",
    "    \n",
    "def plot_svc(svc, X, y):\n",
    "    \"\"\"\n",
    "    plots the results from the sklearn SVM fit\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    svc: instance of sklearn.svm._classes.SVC\n",
    "    X: ndarray of size (M, 2) : data, with M the number of samples and 2 features\n",
    "    y: ndarray of size (M,) :  labels of the datapoints (either 1 or other)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # first plot the data and decisionboundary\n",
    "    plotdata(X, y)\n",
    "    DecisionBoundaryDisplay.from_estimator(clf, X, ax=plt.gca(), alpha=0.4, cmap=plt.cm.Paired, response_method=\"predict\")\n",
    " \n",
    "    # plot the supportvectors\n",
    "    plt.scatter(svc.support_vectors_[:,0], svc.support_vectors_[:,1], c='k', marker='|', s=100, linewidths=1)\n",
    "    plt.title(f'C={svc.C}; Number of support vectors: {svc.support_.size}')    \n",
    "   \n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotdata(X1, y1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "We first make the data set a little simpler: the point on the left changes identity ($1 \\to -1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y1[50] = -1\n",
    "plotdata(X1, y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "This is what SVC from the package does. What is the function of the parameter C?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = SVC(C=1.0, kernel='linear')\n",
    "clf.fit(X1, y1)\n",
    "plot_svc(clf, X1, y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Implement the SVM optimisation yourself.\n",
    "**Implement the optimalisation as as defined below using the genetic algorith that we already have in the run() function. You can also find it in Eq. (4.12) of the lecture notes or (7.3) in Bishop.**\n",
    "\n",
    "Find $\\mathbf{w}$ and $b$ that maximize the margin, while correctly classify all data:<br>\n",
    "  argmax$_{\\mathbf{w},b} \\left \\{ \\frac{1}{\\| \\mathbf{w} \\|} \\displaystyle \\min_m \\left \\{ y^{(m)} \\left ( \\mathbf{w}^T \\mathbf{x}^{(m)} + b \\right) \\right \\} \\right \\},\\;\\; $\n",
    "    $ \\text{subject to }y^{(m)} \\left ( \\mathbf{w}^T \\mathbf{x}^{(m)} + b \\right) \\geq 0$\n",
    "    \n",
    "What is the fitness when the hyperplane of your SVM does not properly classify all points? Add a penelaty term to the fitness function that adds a penealty of size C for each misclassification, or use the hinge loss function (this is the better choice).\n",
    "\n",
    "$\\color{orange}\\text{Write your linear model as we did before}$, inclusing the bias $b$ in the $\\boldsymbol{\\theta}$:\n",
    "$$y=\\mathbf{x}^{\\rm T}\\boldsymbol{\\theta} := \\left ( \\mathbf{w}^T \\mathbf{x} + b \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fSVM(theta,C=1000): \n",
    "    \"\"\"\n",
    "    Computes the fitness of individual theta for a SVM classifier\n",
    "    \n",
    "    See for example the loss function as defined in eq (4.16) of the reader.\n",
    "    Note that we can express fitness=1/loss.\n",
    "    Requires/assumes the presence of the global variables X and y that define the dataset \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : (N+1,) ndarray of float (the genes/parameters of the individual including the bias theta[0] and the N weights theta[1:N]\n",
    "    C     : Scalar for the penalty assigned to points that do not sattisfy the constraint \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    fitness : float\n",
    "        The fitness of theta\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    '''YOUR CODE GOES HERE '''\n",
    "\n",
    "\n",
    "    return fitness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define X and y as required by the fitness function\n",
    "\n",
    "# execute the genetic algorithm\n",
    "\n",
    "\n",
    "# print/plot the results\n",
    "print(f'solution: theta = {theta}')\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "plt.sca(ax1)\n",
    "plt.plot(fitness)\n",
    "plt.xlabel('generation')\n",
    "plt.ylabel('fitness')\n",
    "\n",
    "plt.sca(ax2)\n",
    "plotdata(X1, y1, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "We will now switch back the classification of that single data point. How does this affect your hyperplane?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y1[50] = 1\n",
    "y = y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# execute the genetic algorithm\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "plt.sca(ax1)\n",
    "plt.plot(fitness)\n",
    "plt.xlabel('generation')\n",
    "plt.ylabel('fitness')\n",
    "\n",
    "plt.sca(ax2)\n",
    "plotdata(X1, y1, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### Compare different values of C {1,10,100} and plot the boundaries. Use your run() function to determine the optimal theta's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run minimusation for the 3 C values and plot the boundaries in a subplot with 3 windows. \n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "**Question.** Does the linear case work for C->0 and/or C->infinity work for the case that the dataset is not linearly separable? And what about the genetic code?\n",
    "(set, e.g., the `X1[50,1] = 3` and `y1[50] = 1`)\n",
    "How would you rephrase the genetic optimization such that you could use it for the non separable case as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1[50,1] = 3\n",
    "y1[50] = 1\n",
    "\n",
    "# Run minimusation for 3 C values and plot the boundaries in a subplot with 3 windows. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## SVM with other Kernels **(5EC Mandatory, 3EC recommended)**\n",
    "\n",
    "Below are two more data sets. Use the package sklearn for this. Dive into the manual of this package an figure out how you can get a good classifier. Explain how the methods works with other kernels. Change parameters, such as C, degree and other basis functions: what is their effect? \n",
    "\n",
    "Note the manual of sklearn tells me: '_Regularization parameter. The strength of the regularization is inversely proportional to C._ '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### Example Dataset 2: Material Complexian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = 'SVMdata2.txt'  # points to the datafolder; either a relative or absolute path\n",
    "DELIMITER = ','  # we use a lot of csv or txt files where the data is separated by a comma\n",
    "\n",
    "data = np.loadtxt(FILENAME, delimiter=DELIMITER)\n",
    "X2 = data[:,:2]\n",
    "y2 = data[:,2]\n",
    "plotdata(X2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the sklearn tools to plot a decision boundary based on SVM with at least two different kernels\n",
    "\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### Example Dataset 3: Material Absurdian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = 'SVMdata3.txt'  # points to the datafolder; either a relative or absolute path\n",
    "DELIMITER = ','  # we use a lot of csv or txt files where the data is separated by a comma\n",
    "\n",
    "data = np.loadtxt(FILENAME, delimiter=DELIMITER)\n",
    "X3 = data[:,:2]\n",
    "y3 = data[:,2]\n",
    "plotdata(X3, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the sklearn tools to plot a decision boundary based on SVM with at least two different kernels\n",
    "\n",
    "...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
